{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data for products and assign product names (this should be automated from meta data in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "product1 = pd.read_csv(\"./data/Samsung_chromebook_review.csv\", sep='\\t')\n",
    "product2 = pd.read_csv(\"./data/Ipad_review.csv\", sep='\\t')\n",
    "product3 = pd.read_csv(\"./data/Kindle_fire_review.csv\", sep='\\t')\n",
    "product4 = pd.read_csv(\"./data/Google_nexus_tablet.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product1[\"product_name\"] = \"Samsung Chromebook\"\n",
    "product2[\"product_name\"] = \"iPad Mini\"\n",
    "product3[\"product_name\"] = \"Kindle Fire\"\n",
    "product4[\"product_name\"] = \"Google Nexus\"\n",
    "\n",
    "frames = [product1, product2, product3, product4]\n",
    "all_reviews = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work out number of product reviews for each product. Maybe this will be useful at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_product_reviews = all_reviews.groupby([all_reviews.product_name]).overall.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import nltk.data\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#Converts individual review text to text blobs. Each part of blobs list in a sentence\n",
    "def review_to_blobs( review, tokenizer ):\n",
    "    blobs = []\n",
    "    \n",
    "    if type(review) != str:\n",
    "        print(type(review))\n",
    "    sentences = tokenizer.tokenize(review.strip())\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > 0:\n",
    "            sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "            blobs.append(TextBlob(sentence))\n",
    "    return blobs\n",
    "\n",
    "#Makes list of text blobs (each part of list is a review).\n",
    "def product_to_blobs(popular_product):\n",
    "    blobs = []\n",
    "\n",
    "    print(\"Parsing sentences from training set\")\n",
    "    icount = 1\n",
    "    for review in popular_product[\"reviewText\"]:\n",
    "        icount += 1\n",
    "        if icount%1000 == 0:\n",
    "            print(\"Cleaning and tokenizing review\", icount, \"of\", len(popular_product))\n",
    "        if type(review) == str:\n",
    "            blobs += review_to_blobs(review, tokenizer)\n",
    "    return blobs\n",
    "\n",
    "#Sort review sentences into positive and negative and find noun phrases\n",
    "def sort_sentiment_phrases(blobs):\n",
    "    positive_sentences = []\n",
    "    negative_sentences = []\n",
    "\n",
    "    positive_noun_phrases = []\n",
    "    negative_noun_phrases = []\n",
    "    for blob in blobs:\n",
    "        if blob.polarity > 0.3:\n",
    "            positive_sentences.append(blob)\n",
    "            positive_noun_phrases.append(blob.noun_phrases)\n",
    "        elif blob.polarity < -0.3:\n",
    "            negative_sentences.append(blob)\n",
    "            negative_noun_phrases.append(blob.noun_phrases)\n",
    "            \n",
    "    negative_phrases = []\n",
    "    for noun_phrases in negative_noun_phrases:\n",
    "        for noun_phrase in noun_phrases:\n",
    "            negative_phrases.append(noun_phrase)\n",
    "        \n",
    "    positive_phrases = []\n",
    "    for noun_phrases in positive_noun_phrases:\n",
    "        for noun_phrase in noun_phrases:\n",
    "            positive_phrases.append(noun_phrase)\n",
    "        \n",
    "    return positive_phrases, negative_phrases\n",
    "\n",
    "#Find frequency of the noun phrases\n",
    "def get_word_freq(words):\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 1000, ngram_range=(2,3)) \n",
    "    \n",
    "    word_features = vectorizer.fit_transform(words)\n",
    "    word_features = word_features.toarray()\n",
    "    \n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    \n",
    "    dist = np.sum(word_features, axis=0)\n",
    "\n",
    "    word_freq = {'count': dist, 'vocab': vocab}\n",
    "    word_freq = pd.DataFrame(word_freq)\n",
    "    word_freq = word_freq.sort_values(by=\"count\",ascending=False)\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function returns top five negative and positive words for a given product\n",
    "\n",
    "def get_top_five_phrases(products):\n",
    "    if type(products) == str:\n",
    "        reviews = all_reviews[all_reviews[\"product_name\"] == products]\n",
    "        \n",
    "        blobs = product_to_blobs(reviews)\n",
    "        positive_phrases, negative_phrases = sort_sentiment_phrases(blobs)\n",
    "        freq_pos = get_word_freq(positive_phrases)\n",
    "        freq_neg = get_word_freq(negative_phrases)\n",
    "        \n",
    "        top_five_pos = freq_pos.head(5)\n",
    "        top_five_neg = freq_neg.head(5)\n",
    "        \n",
    "        return top_five_pos, top_five_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Cleaning and tokenizing review 1000 of 1933\n"
     ]
    }
   ],
   "source": [
    "pos, neg = get_top_five_phrases(\"iPad Mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
